{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Plant Phenomics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1O-xoLgbExcAI8aiSzstFxuvFyJ-wKgfc",
      "authorship_tag": "ABX9TyOlyUgiANTRsXjsIZ7gOmhn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohsin1931/Plant_Phenotyping/blob/main/Deep_Plant_Phenomics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCAtUJxYfMHi",
        "outputId": "363d3ce1-a212-4c36-f7d2-05c22c5646b2"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive/')\r\n",
        "\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2odPrgp4f8Jn"
      },
      "source": [
        "pip install '/content/drive/My Drive/deepplantphenomics'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzXavNsihBxv",
        "outputId": "50dcb3ad-4b9d-4953-c8be-f6e71d8f2227"
      },
      "source": [
        "import deepplantphenomics as dpp\r\n",
        "\r\n",
        "model = dpp.RegressionModel(save_dir='/content/drive/MyDrive/Colab Notebooks/', debug=True, save_checkpoints=False, tensorboard_dir='/content/drive/My Drive/tensorlogs', report_rate=20)\r\n",
        "\r\n",
        "#3 channels for colour , 1 fro greyscale\r\n",
        "channels = 3 \r\n",
        "\r\n",
        "#setup and hyperparameters\r\n",
        "model.set_batch_size(4)\r\n",
        "model.set_number_of_threads(8)\r\n",
        "model.set_image_dimensions(128,128,channels)\r\n",
        "model.set_resize_images(True)\r\n",
        "\r\n",
        "#hyperparameters for training\r\n",
        "model.set_num_regression_outputs(1)\r\n",
        "model.set_test_split(0.2)\r\n",
        "model.set_validation_split(0.0)\r\n",
        "model.set_learning_rate(0.0001)\r\n",
        "model.set_weight_initializer('xavier')\r\n",
        "model.set_maximum_training_epochs(500)\r\n",
        "\r\n",
        "model.force_split_shuffle(True)\r\n",
        "\r\n",
        "#Augmentation option\r\n",
        "model.set_augmentation_brightness_and_contrast(True)\r\n",
        "model.set_augmentation_flip_horizontal(True)\r\n",
        "model.set_augmentation_flip_vertical(True)\r\n",
        "model.set_augmentation_crop(True)\r\n",
        "\r\n",
        "\r\n",
        "#Loading all the data\r\n",
        "model.load_ippn_leaf_count_dataset_from_directory('/content/drive/My Drive/ColabNotebooks/Plant_Phenotyping_Datasets/Plant/Ara2013-Canon')\r\n",
        "\r\n",
        "#Defining the model architecture\r\n",
        "model.add_input_layer()\r\n",
        "\r\n",
        "model.add_convolutional_layer(filter_dimension=[5, 5, channels, 32], stride_length=1, activation_function='tanh')\r\n",
        "model.add_pooling_layer(kernel_size=3, stride_length=2)\r\n",
        "\r\n",
        "model.add_convolutional_layer(filter_dimension=[5,5,32,64], stride_length=1, activation_function='tanh')\r\n",
        "model.add_pooling_layer(kernel_size=3, stride_length=2)\r\n",
        "\r\n",
        "model.add_convolutional_layer(filter_dimension=[3,3,64,64], stride_length=1, activation_function='tanh')\r\n",
        "model.add_pooling_layer(kernel_size=3, stride_length=2)\r\n",
        "\r\n",
        "model.add_convolutional_layer(filter_dimension=[3,3,64,64], stride_length=1, activation_function='tanh')\r\n",
        "model.add_pooling_layer(kernel_size=3, stride_length=2)\r\n",
        "\r\n",
        "model.add_output_layer()\r\n",
        "\r\n",
        "model.begin_training()\r\n",
        "\r\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01:58PM: TensorFlow loaded...\n",
            "01:58PM: Total raw examples is 165\n",
            "01:58PM: Parsing dataset...\n",
            "01:58PM: Adding the input layer...\n",
            "01:58PM: Adding convolutional layer conv1...\n",
            "01:58PM: Filter dimensions: [5, 5, 3, 32] Outputs: [4, 96, 96, 32]\n",
            "01:58PM: Adding pooling layer pool1...\n",
            "01:58PM: Outputs: [4, 48, 48, 32]\n",
            "01:58PM: Adding convolutional layer conv2...\n",
            "01:58PM: Filter dimensions: [5, 5, 32, 64] Outputs: [4, 48, 48, 64]\n",
            "01:58PM: Adding pooling layer pool2...\n",
            "01:58PM: Outputs: [4, 24, 24, 64]\n",
            "01:58PM: Adding convolutional layer conv3...\n",
            "01:58PM: Filter dimensions: [3, 3, 64, 64] Outputs: [4, 24, 24, 64]\n",
            "01:58PM: Adding pooling layer pool3...\n",
            "01:58PM: Outputs: [4, 12, 12, 64]\n",
            "01:58PM: Adding convolutional layer conv4...\n",
            "01:58PM: Filter dimensions: [3, 3, 64, 64] Outputs: [4, 12, 12, 64]\n",
            "01:58PM: Adding pooling layer pool4...\n",
            "01:58PM: Outputs: [4, 6, 6, 64]\n",
            "01:58PM: Adding output layer...\n",
            "01:58PM: Inputs: [4, 6, 6, 64] Outputs: 1\n",
            "01:58PM: Assembling graph...\n",
            "01:58PM: Graph: Parsing dataset...\n",
            "01:58PM: Building new partition mask.\n",
            "01:58PM: Total training samples is 132\n",
            "01:58PM: Total validation samples is 0\n",
            "01:58PM: Total testing samples is 33\n",
            "01:58PM: Batches per epoch: 33.000000\n",
            "01:58PM: Running to 16500 batches\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/image_ops_impl.py:1518: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deepplantphenomics/deepplantpheno.py:773: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
            "01:58PM: Using Adam optimizer\n",
            "01:58PM: Graph: Creating layer parameters...\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "01:58PM: Graph: Calculating loss and gradients...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "01:58PM: Creating Tensorboard summaries...\n",
            "01:58PM: Assembled the graph\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/16500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:58PM: Initializing parameters...\n",
            "01:58PM: Beginning training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "03:05PM: Results for batch 16480 (epoch 499.4) - Loss: [0.27681503], samples/sec: 23.95: 100%|██████████| 16500/16500 [1:07:03<00:00,  4.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "03:05PM: Stopping due to maximum epochs\n",
            "03:05PM: Saving parameters...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 11%|█         | 1/9 [00:00<00:00,  8.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "03:05PM: Computing total test accuracy/regression loss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 17.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "03:05PM: Mean loss: 0.30655309557914734\n",
            "03:05PM: Loss standard deviation: 1.087814211845398\n",
            "03:05PM: Mean absolute loss: 0.868243932723999\n",
            "03:05PM: Absolute loss standard deviation: 0.7235100865364075\n",
            "03:05PM: Min error: -2.239828109741211\n",
            "03:05PM: Max error: 2.6441822052001953\n",
            "03:05PM: MSE: 1.2773144245147705\n",
            "03:05PM: R^2: 0.6238519847393036\n",
            "03:05PM: All test labels:\n",
            "03:05PM: [[ 8.]\n",
            " [11.]\n",
            " [11.]\n",
            " [11.]\n",
            " [ 8.]\n",
            " [11.]\n",
            " [10.]\n",
            " [10.]\n",
            " [10.]\n",
            " [ 9.]\n",
            " [11.]\n",
            " [11.]\n",
            " [11.]\n",
            " [ 7.]\n",
            " [ 5.]\n",
            " [ 6.]\n",
            " [ 6.]\n",
            " [ 8.]\n",
            " [ 6.]\n",
            " [ 7.]\n",
            " [ 8.]\n",
            " [ 6.]\n",
            " [ 6.]\n",
            " [ 9.]\n",
            " [ 7.]\n",
            " [ 9.]\n",
            " [ 7.]\n",
            " [ 8.]\n",
            " [ 7.]\n",
            " [10.]\n",
            " [ 7.]\n",
            " [ 9.]\n",
            " [ 8.]]\n",
            "03:05PM: All predictions:\n",
            "03:05PM: [[ 8.730485 ]\n",
            " [11.258721 ]\n",
            " [10.876779 ]\n",
            " [11.352097 ]\n",
            " [ 9.782083 ]\n",
            " [10.685304 ]\n",
            " [ 7.760172 ]\n",
            " [ 9.542701 ]\n",
            " [ 9.968902 ]\n",
            " [11.644182 ]\n",
            " [12.855468 ]\n",
            " [11.5963335]\n",
            " [13.238334 ]\n",
            " [ 6.2881136]\n",
            " [ 5.1761246]\n",
            " [ 7.334511 ]\n",
            " [ 5.035137 ]\n",
            " [ 7.93638  ]\n",
            " [ 5.2798676]\n",
            " [ 8.0774765]\n",
            " [ 9.556535 ]\n",
            " [ 6.512025 ]\n",
            " [ 5.376724 ]\n",
            " [ 8.823232 ]\n",
            " [ 7.5662794]\n",
            " [ 7.867711 ]\n",
            " [ 6.7349005]\n",
            " [ 6.556179 ]\n",
            " [ 7.207533 ]\n",
            " [10.389049 ]\n",
            " [ 7.396915 ]\n",
            " [ 9.551865 ]\n",
            " [10.158136 ]]\n",
            "03:05PM: Histogram of l2 losses:\n",
            "03:05PM: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 2 0 1 0 0 1\n",
            " 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 0 3 0 0 1 2 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1]\n",
            "03:05PM: Shutdown requested, ending session...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_esS5lGpDKF"
      },
      "source": [
        "class leafRegressor(object):\r\n",
        "  model = None\r\n",
        "\r\n",
        "  img_height = 128\r\n",
        "  img_width = 128\r\n",
        "  _dir_name = '/content/drive/MyDrive/leaf_regressor'\r\n",
        "\r\n",
        "  def __init__(self, batch_size=8):\r\n",
        "     import deepplantphenomics as dpp\r\n",
        "     self.model = dpp.RegressionModel(debug=False, load_from_saved=self._dir_name)\r\n",
        "\r\n",
        "     #model hyper parameters\r\n",
        "     self.model.set_batch_size(batch_size)\r\n",
        "     self.model.set_number_of_threads(1)\r\n",
        "     self.model.set_image_dimensions(self.img_height,self.img_width, 3)\r\n",
        "     self.model.set_resize_images(True)\r\n",
        "     self.model.set_augmentation_crop(True)\r\n",
        "\r\n",
        "     #model architecture\r\n",
        "     self.model.add_input_layer()\r\n",
        "\r\n",
        "     self.model.add_convolutional_layer(filter_dimension=[5, 5, channels, 32], stride_length=1, activation_function='tanh')\r\n",
        "     self.model.add_pooling_layer(kernel_size=3, stride_length=2)\r\n",
        "\r\n",
        "     self.model.add_convolutional_layer(filter_dimension=[5,5,32,64], stride_length=1, activation_function='tanh')\r\n",
        "     self.model.add_pooling_layer(kernel_size=3, stride_length=2)\r\n",
        "\r\n",
        "     self.model.add_convolutional_layer(filter_dimension=[3,3,64,64], stride_length=1, activation_function='tanh')\r\n",
        "     self.model.add_pooling_layer(kernel_size=3, stride_length=2)\r\n",
        "\r\n",
        "     self.model.add_convolutional_layer(filter_dimension=[3,3,64,64], stride_length=1, activation_function='tanh')\r\n",
        "     self.model.add_pooling_layer(kernel_size=3, stride_length=2)\r\n",
        "\r\n",
        "     self.model.add_output_layer()\r\n",
        "  \r\n",
        "  def forward_pass(self, x):\r\n",
        "  \r\n",
        "    y = self.model.forward_pass_with_file_inputs(x)\r\n",
        "    return y\r\n",
        "\r\n",
        "  def shut_down(self):\r\n",
        "    self.model.shut_down()\r\n",
        "\r\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4D9YPCSxk3M",
        "outputId": "14c241b1-7ed5-453b-9be6-eab37923dbfb"
      },
      "source": [
        "images = ['/content/drive/MyDrive/ColabNotebooks/Plant_Phenotyping_Datasets/Test/Plant_1.png','/content/drive/MyDrive/ColabNotebooks/Plant_Phenotyping_Datasets/Test/Plant_2.png','/content/drive/MyDrive/ColabNotebooks/Plant_Phenotyping_Datasets/Test/Plant_3.png']\r\n",
        "print(\"Performing leaf estimation\")\r\n",
        "\r\n",
        "net = leafRegressor()\r\n",
        "\r\n",
        "leaf_counts = net.forward_pass(images)\r\n",
        "net.shut_down()\r\n",
        "\r\n",
        "for k,v in zip(images, leaf_counts):\r\n",
        "  print('%s : %d' % (k,v))\r\n",
        "\r\n",
        "print('done')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performing leaf estimation\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/Colab Notebooks//saved_state/tfhSaved\n",
            "/content/drive/MyDrive/ColabNotebooks/Plant_Phenotyping_Datasets/Test/Plant_1.png : 7\n",
            "/content/drive/MyDrive/ColabNotebooks/Plant_Phenotyping_Datasets/Test/Plant_2.png : 7\n",
            "/content/drive/MyDrive/ColabNotebooks/Plant_Phenotyping_Datasets/Test/Plant_3.png : 10\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}